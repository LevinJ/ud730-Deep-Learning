problem 1: l2 regularization

logistic regression, Beta  = 0.003

Minibatch loss at step 3000: 0.717211
Minibatch accuracy: 84.4%
Validation accuracy: 81.8%
Test accuracy: 89.0%
end timer
Duration: 0 hours, 0 minutes and 13 seconds


one hidden relu layer network, Beta = 5e-4 

Minibatch loss at step 3000: 2.166549
Minibatch accuracy: 84.4%
Validation accuracy: 82.5%
Test accuracy: 89.7%
end timer
Duration: 0 hours, 3 minutes and 12 seconds



one hidden relu layer network Beta  = 0.001

Minibatch loss at step 3000: 15.506946
Minibatch accuracy: 87.5%
Validation accuracy: 86.4%
Test accuracy: 93.0%
end timer
Duration: 0 hours, 2 minutes and 54 seconds



problem 3, relu with dropout

Minibatch loss at step 3000: 3.628932
Minibatch accuracy: 78.9%
Validation accuracy: 79.6%
Test accuracy: 87.0%
end timer
Duration: 0 hours, 2 minutes and 36 seconds

problem 4, mulitlayer:

Minibatch loss at step 3000: 0.157209, no dropout
Minibatch accuracy: 94.5%
Validation accuracy: 89.0%
Test accuracy: 94.5%
Incorrectly labelled test sample number: 1025.0
end timer
Duration: 0 hours, 6 minutes and 12 seconds


Minibatch loss at step 100000: 0.061844
Minibatch accuracy: 97.7%
Validation accuracy: 90.8%
Test accuracy: 96.0%
end timer
Duration: 3 hours, 0 minutes and 37 seconds

multipile layer with dropout and learnig decay
DEBUG:root:Minibatch loss at step 100000/100001: 0.399770
DEBUG:root:Minibatch accuracy: 90.6%
DEBUG:root:Validation accuracy: 90.5%
DEBUG:root:Test accuracy: 95.9%
DEBUG:root:Incorrectly labelled test sample number: 776.0
DEBUG:root:end timer
DEBUG:root:Duration: 7 hours, 50 minutes and 35 seconds
